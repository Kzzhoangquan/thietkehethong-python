import torch
import cv2
import numpy as np
from ultralytics import YOLO
from transformers import TimesformerForVideoClassification, AutoImageProcessor

# Load m√¥ h√¨nh ph√°t hi·ªán ng∆∞·ªùi (YOLOv8)
yolo_model = YOLO("yolov8n.pt")  # C√≥ th·ªÉ thay b·∫±ng yolov8s.pt ƒë·ªÉ ch√≠nh x√°c h∆°n

# Load m√¥ h√¨nh nh·∫≠n d·∫°ng h√†nh ƒë·ªông (TimeSformer)
action_model_name = "facebook/timesformer-base-finetuned-k400"
action_model = TimesformerForVideoClassification.from_pretrained(action_model_name)
processor = AutoImageProcessor.from_pretrained(action_model_name)
action_model.eval()

# Load nh√£n h√†nh ƒë·ªông c·ªßa Kinetics-400
with open("kinetics_400_labels.txt", "r") as f:
    action_labels = [line.strip() for line in f.readlines()]

# H√†m ph√°t hi·ªán ng∆∞·ªùi trong khung h√¨nh
def detect_people(frame):
    results = yolo_model(frame)
    persons = []
    
    for result in results:
        for box in result.boxes:
            cls = int(box.cls.item())  # L·∫•y class
            if cls == 0:  # Class 0 l√† "person" trong YOLO
                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                persons.append((x1, y1, x2, y2))
    
    return persons

# H√†m hi·ªÉn th·ªã video v·ªõi khung ng∆∞·ªùi
def show_video(video_path):
    cap = cv2.VideoCapture(video_path)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        persons = detect_people(frame)

        for x1, y1, x2, y2 in persons:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # V·∫Ω khung xanh

        cv2.imshow("Video", frame)
        if cv2.waitKey(25) & 0xFF == ord('q'):  # Nh·∫•n 'q' ƒë·ªÉ tho√°t
            break

    cap.release()
    cv2.destroyAllWindows()

# H√†m tr√≠ch xu·∫•t frames c·ªßa t·ª´ng ng∆∞·ªùi
def extract_frames(video_path, persons, num_frames=8):
    cap = cv2.VideoCapture(video_path)
    frames_dict = {i: [] for i in range(len(persons))}

    for _ in range(num_frames):
        ret, frame = cap.read()
        if not ret:
            break

        for i, (x1, y1, x2, y2) in enumerate(persons):
            person_crop = frame[y1:y2, x1:x2]
            person_crop = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)
            frames_dict[i].append(person_crop)

    cap.release()
    return frames_dict

# Nh·∫≠n d·∫°ng h√†nh ƒë·ªông sau khi video ch·∫°y xong
def recognize_actions(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()
    if not ret:
        print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc video!")
        return

    # Ph√°t hi·ªán ng∆∞·ªùi trong khung ƒë·∫ßu ti√™n
    persons = detect_people(first_frame)
    if not persons:
        print("‚ùå Kh√¥ng ph√°t hi·ªán ng∆∞·ªùi n√†o!")
        return

    print(f"üë• Ph√°t hi·ªán {len(persons)} ng∆∞·ªùi trong video.")
    
    # Hi·ªÉn th·ªã video v·ªõi khung tr∆∞·ªõc khi nh·∫≠n d·∫°ng
    show_video(video_path)

    # Tr√≠ch xu·∫•t frames sau khi video ch·∫°y xong
    frames_dict = extract_frames(video_path, persons, num_frames=8)
    actions = {}

    # Nh·∫≠n d·∫°ng h√†nh ƒë·ªông c·ªßa t·ª´ng ng∆∞·ªùi
    for person_id, frames in frames_dict.items():
        if len(frames) < 8:
            print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªß frames cho ng∆∞·ªùi {person_id}, b·ªè qua...")
            continue
        
        inputs = processor(images=frames, return_tensors="pt")
        
        with torch.no_grad():
            outputs = action_model(**inputs)

        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
        action = action_labels[predicted_class]
        actions[person_id] = action

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ sau khi video k·∫øt th√∫c
    print("üìå K·∫øt qu·∫£ d·ª± ƒëo√°n:")
    for person_id, action in actions.items():
        print(f"üßç Ng∆∞·ªùi {person_id}: {action}")

# Ch·∫°y nh·∫≠n d·∫°ng
if __name__ == "__main__":
    video_path = "1.mp4"  # ƒê·ªïi th√†nh ƒë∆∞·ªùng d·∫´n video
    recognize_actions(video_path)
