import base64
from flask import Flask, request, jsonify
import os
import cv2
import torch
import numpy as np
from transformers import TimesformerForVideoClassification, AutoImageProcessor
from flask_cors import CORS  # Há»— trá»£ CORS
from io import BytesIO
from PIL import Image

app = Flask(__name__)
CORS(app)  # Báº­t CORS cho táº¥t cáº£ cÃ¡c route

# Load mÃ´ hÃ¬nh Ä‘Ã£ train
device = "cuda" if torch.cuda.is_available() else "cpu"
model = TimesformerForVideoClassification.from_pretrained("custom_timesformer").to(device)
processor = AutoImageProcessor.from_pretrained("custom_timesformer")

# NhÃ£n hÃ nh Ä‘á»™ng
labels = ["Ä‘áº¥m", "Ä‘Ã¡", "tÃ¡t"]

# HÃ m trÃ­ch xuáº¥t frames tá»« video
def extract_frames(video_path, num_frames=8):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total_frames < num_frames:
        print("âš  Video quÃ¡ ngáº¯n!")
        return None

    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frames = []

    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame)

    cap.release()
    return frames if len(frames) == num_frames else None

# API nháº­n video vÃ  dá»± Ä‘oÃ¡n
# @app.route('/predict', methods=['POST'])
# def predict_action():
#     if 'video' not in request.files:
#         return jsonify({"error": "KhÃ´ng cÃ³ file video"}), 400

#     video_file = request.files['video']
#     video_path = "temp_video.mp4"
#     video_file.save(video_path)

#     frames = extract_frames(video_path, num_frames=8)
#     if frames is None:
#         return jsonify({"error": "Video quÃ¡ ngáº¯n hoáº·c lá»—i khi trÃ­ch xuáº¥t frames"}), 400

#     inputs = processor(images=frames, return_tensors="pt").to(device)
    
#     with torch.no_grad():
#         outputs = model(**inputs)

#     predicted_class = torch.argmax(outputs.logits, dim=1).item()
#     action = labels[predicted_class] if 0 <= predicted_class < len(labels) else "KhÃ´ng xÃ¡c Ä‘á»‹nh"

#     return jsonify({"action": action})

UPLOAD_FOLDER = "static/uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
@app.route('/predict', methods=['POST'])
def predict_action():
    if 'video' not in request.files:
        return jsonify({"error": "KhÃ´ng cÃ³ file video"}), 400

    video_file = request.files['video']
    
    # Táº¡o tÃªn file duy nháº¥t
    video_filename = f"video.mp4"
    video_path = os.path.join(UPLOAD_FOLDER, video_filename)
    
    video_file.save(video_path)

    # Hiá»ƒn thá»‹ video trÆ°á»›c khi nháº­n dáº¡ng
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return jsonify({"error": "KhÃ´ng thá»ƒ má»Ÿ video"}), 400

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break  # Káº¿t thÃºc video

        cv2.imshow("Video TrÆ°á»›c Khi Dá»± ÄoÃ¡n", frame)
        if cv2.waitKey(25) & 0xFF == ord('q'):  # Nháº¥n 'q' Ä‘á»ƒ thoÃ¡t
            break

    cap.release()
    cv2.destroyAllWindows()

    # Tiáº¿n hÃ nh nháº­n dáº¡ng sau khi hiá»ƒn thá»‹ video
    frames = extract_frames(video_path, num_frames=8)
    if frames is None:
        return jsonify({"error": "Video quÃ¡ ngáº¯n hoáº·c lá»—i khi trÃ­ch xuáº¥t frames"}), 400

    inputs = processor(images=frames, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**inputs)

    predicted_class = torch.argmax(outputs.logits, dim=1).item()
    action = labels[predicted_class] if 0 <= predicted_class < len(labels) else "khong xac dinh"
    print(action)

    return jsonify({"action": action})

def preprocess_images(images_base64):
    """Giáº£i mÃ£ danh sÃ¡ch áº£nh tá»« base64 vÃ  chuyá»ƒn thÃ nh tensor"""
    frames = []
    for img_base64 in images_base64:
        image_data = base64.b64decode(img_base64.split(",")[1])
        image = Image.open(BytesIO(image_data)).convert("RGB")
        frames.append(np.array(image))
    
    # Äáº£m báº£o cÃ³ Ä‘Ãºng 8 frames
    if len(frames) != 8:
        return None
    
    # Tiá»n xá»­ lÃ½ frames Ä‘á»ƒ Ä‘Æ°a vÃ o model
    inputs = processor(images=frames, return_tensors="pt")
    return inputs["pixel_values"]

@app.route("/predict_camera", methods=["POST"])
def predict_camera():
    print("ðŸ“¡ Nháº­n request tá»« client...")

    # Kiá»ƒm tra cÃ³ file hay khÃ´ng
    if "file" not in request.files:
        return jsonify({"error": "KhÃ´ng tÃ¬m tháº¥y file"}), 400

    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "TÃªn file khÃ´ng há»£p lá»‡"}), 400

    print(f"âœ… ÄÃ£ nháº­n file: {file.filename}")

    # LÆ°u file video táº¡m thá»i
    video_path = os.path.join(UPLOAD_FOLDER, "camera_video.mp4")
    file.save(video_path)

    # TrÃ­ch xuáº¥t frames tá»« video
    frames = extract_frames(video_path, num_frames=8)
    if frames is None:
        return jsonify({"error": "Video quÃ¡ ngáº¯n hoáº·c lá»—i khi trÃ­ch xuáº¥t frames"}), 400

    # Tiá»n xá»­ lÃ½ frames Ä‘á»ƒ Ä‘Æ°a vÃ o model
    inputs = processor(images=frames, return_tensors="pt").to(device)

    # Dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng
    with torch.no_grad():
        outputs = model(**inputs)

    predicted_class = torch.argmax(outputs.logits, dim=1).item()
    action = labels[predicted_class] if 0 <= predicted_class < len(labels) else "KhÃ´ng xÃ¡c Ä‘á»‹nh"
    
    print(f"ðŸŽ¯ HÃ nh Ä‘á»™ng nháº­n diá»‡n: {action}")
    return jsonify({"action": action})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)

# import base64
# from collections import deque
# from flask import Flask, request, jsonify
# import os
# import cv2
# import torch
# import numpy as np
# from transformers import TimesformerForVideoClassification, AutoImageProcessor
# from flask_cors import CORS  # Há»— trá»£ CORS
# from io import BytesIO
# from PIL import Image
# import torchvision.transforms as T


# app = Flask(__name__)
# CORS(app)  # Báº­t CORS cho táº¥t cáº£ cÃ¡c route

# # Load mÃ´ hÃ¬nh Ä‘Ã£ train
# device = "cuda" if torch.cuda.is_available() else "cpu"
# model = TimesformerForVideoClassification.from_pretrained("custom_timesformer").to(device)
# processor = AutoImageProcessor.from_pretrained("custom_timesformer")

# # NhÃ£n hÃ nh Ä‘á»™ng
# labels = ["Ä‘áº¥m", "Ä‘Ã¡", "tÃ¡t"]

# frame_buffer = deque(maxlen=15)

# def save_video(frames, filename="output.avi"):
#     """LÆ°u 15 frame thÃ nh video náº¿u phÃ¡t hiá»‡n hÃ nh Ä‘á»™ng."""
#     height, width, _ = frames[0].shape  # ThÃªm _ Ä‘á»ƒ láº¥y sá»‘ kÃªnh mÃ u
#     out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'XVID'), 10, (width, height), isColor=True)

#     for frame in frames:
#         out.write(frame)

#     out.release()

# @app.route("/upload_frame", methods=["POST"])
# def upload_frame():
#     global frame_buffer
    
#     # Nháº­n danh sÃ¡ch frame tá»« Java
#     files = request.files.getlist("frames")
#     if not files:
#         return jsonify({"error": "No frames received"}), 400
    
#     # Xá»­ lÃ½ tá»«ng frame
#     for file in files:
#         nparr = np.frombuffer(file.read(), np.uint8)
#         img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)  # Äá»c áº£nh cÃ³ mÃ u
#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Chuyá»ƒn BGR â†’ RGB
#         img = cv2.resize(img, (224, 224))  # Resize áº£nh vá» 224x224
#         frame_buffer.append(img)
    
#     # Náº¿u cÃ³ Ä‘á»§ 15 frame, thá»±c hiá»‡n dá»± Ä‘oÃ¡n
#     if len(frame_buffer) == 15:
#         try:
#             # Äáº§u tiÃªn, chuyá»ƒn Ä‘á»•i frames thÃ nh Ä‘á»‹nh dáº¡ng Ä‘áº·c biá»‡t cho TimesformerForVideoClassification
#             # Timesformer mong Ä‘á»£i Ä‘áº§u vÃ o lÃ  tensor cÃ³ hÃ¬nh dáº¡ng (batch_size, num_frames, num_channels, height, width)
#             frames_array = np.array(frame_buffer)  # Shape: (15, 224, 224, 3)
#             # Chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng (1, 15, 3, 224, 224)
#             video_tensor = torch.from_numpy(frames_array).float() / 255.0
#             video_tensor = video_tensor.permute(0, 3, 1, 2)  # (15, 3, 224, 224)
#             video_tensor = video_tensor.unsqueeze(0)  # (1, 15, 3, 224, 224)
#             video_tensor = video_tensor.to(device)
            
#             # Gá»i mÃ´ hÃ¬nh vá»›i tham sá»‘ Ä‘Ãºng
#             with torch.no_grad():
#                 outputs = model(pixel_values=video_tensor)
#                 predicted_class = torch.argmax(outputs.logits, dim=1).item()
#                 action = labels[predicted_class]
            
#             # LÆ°u video náº¿u cÃ³ hÃ nh Ä‘á»™ng
#             save_video(list(frame_buffer), f"action_{action}.avi")
            
#             # Reset buffer Ä‘á»ƒ nháº­n tiáº¿p 15 frame má»›i
#             frame_buffer.clear()
            
#             return jsonify({"action": action})
#         except Exception as e:
#             import traceback
#             traceback_str = traceback.format_exc()
#             print(f"Error: {str(e)}\n{traceback_str}")
#             return jsonify({"error": str(e), "traceback": traceback_str}), 500
    
#     # Tráº£ vá» thÃ´ng bÃ¡o náº¿u chÆ°a Ä‘á»§ 15 frame Ä‘á»ƒ dá»± Ä‘oÃ¡n
#     return jsonify({"action": "waiting"})  # ChÆ°a Ä‘á»§ 15 frame Ä‘á»ƒ dá»± Ä‘oÃ¡n



# if __name__ == '__main__':
#     app.run(host='0.0.0.0', port=5000, debug=True)

