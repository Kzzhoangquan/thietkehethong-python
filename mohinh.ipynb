{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ Cáº­p Nháº­t Dataset & Huáº¥n Luyá»‡n TimeSformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Epoch 1/5 - Loss: 143.0471\n",
      "ğŸ”„ Epoch 2/5 - Loss: 29.8161\n",
      "ğŸ”„ Epoch 3/5 - Loss: 14.9409\n",
      "ğŸ”„ Epoch 4/5 - Loss: 7.8184\n",
      "ğŸ”„ Epoch 5/5 - Loss: 4.7769\n",
      "âœ… Huáº¥n luyá»‡n xong! ÄÃ£ lÆ°u mÃ´ hÃ¬nh táº¡i custom_timesformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "model_name = \"facebook/timesformer-base-finetuned-k400\"\n",
    "model = TimesformerForVideoClassification.from_pretrained(model_name, ignore_mismatched_sizes=True)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "labels = {\"Ä‘áº¥m\": 0, \"Ä‘Ã¡\": 1, \"tÃ¡t\": 2} \n",
    "\n",
    "def extract_frames(video_path, num_frames=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames < num_frames:\n",
    "        print(f\"âš  Video {video_path} quÃ¡ ngáº¯n!\")\n",
    "        return None\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Chuyá»ƒn vá» RGB\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames if len(frames) == num_frames else None\n",
    "\n",
    "class ActionVideoDataset(Dataset):\n",
    "    def __init__(self, data_folder, labels):\n",
    "        self.data_folder = data_folder\n",
    "        self.labels = labels\n",
    "        self.video_paths = []\n",
    "        self.targets = []\n",
    "\n",
    "        for action, label in labels.items():\n",
    "            action_folder = os.path.join(data_folder, action)\n",
    "            if os.path.exists(action_folder):\n",
    "                for video in os.listdir(action_folder):\n",
    "                    if video.endswith(\".mp4\"):\n",
    "                        video_path = os.path.join(action_folder, video)\n",
    "                        self.video_paths.append(video_path)\n",
    "                        self.targets.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        frames = extract_frames(video_path, num_frames=8)\n",
    "        if frames is None:\n",
    "            return None\n",
    "\n",
    "        inputs = processor(images=frames, return_tensors=\"pt\")\n",
    "        return inputs[\"pixel_values\"].squeeze(0), torch.tensor(label)\n",
    "\n",
    "\n",
    "train_dataset = ActionVideoDataset(\"data\", labels)\n",
    "train_dataset = [d for d in train_dataset if d is not None] \n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        loss = loss_fn(outputs.logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"ğŸ”„ Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# LÆ°u mÃ´ hÃ¬nh\n",
    "model.save_pretrained(\"custom_timesformer\")\n",
    "processor.save_pretrained(\"custom_timesformer\")\n",
    "print(\"âœ… Huáº¥n luyá»‡n xong! ÄÃ£ lÆ°u mÃ´ hÃ¬nh táº¡i custom_timesformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ Cáº­p Nháº­t Code Dá»± ÄoÃ¡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang xá»­ lÃ½ video: V_797.mp4\n",
      "âœ… ÄÃ£ tiá»n xá»­ lÃ½ xong, báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n...\n",
      "ğŸ”¢ GiÃ¡ trá»‹ logits: tensor([[13.5167, 10.6946,  6.0627,  2.0314, -0.1815,  1.3605, -1.0458, -1.6472,\n",
      "          0.5926, -0.2519, -2.9618, -2.2580,  2.1829,  1.4996, -0.1884, -1.4981,\n",
      "          3.3528, -1.1448, -0.2744, -1.7093, -3.5220,  0.9008, -2.2963,  2.6589,\n",
      "         -0.5216,  2.2699,  0.8419, -1.0576, -2.9878, -1.3719, -2.8797, -2.8307,\n",
      "         -0.7263, -0.2985, -0.8822,  0.0927, -0.4938,  1.1167, -0.4299, -1.0720,\n",
      "         -0.2360,  5.0088, -1.1312,  0.3184,  2.8360, -4.9479, -2.3153, -1.7634,\n",
      "         -0.6309, -0.3694,  0.1598,  2.6668, -0.7356,  4.5414, -0.8498, -1.2402,\n",
      "          0.5067,  1.3860, -3.7504, -0.7423, -2.1778,  1.9309, -1.7978,  0.3151,\n",
      "         -2.2947,  4.4684,  3.9735,  2.1701,  1.7811, -0.4944, -2.0105, -0.2254,\n",
      "         -0.8658, -2.3823,  0.7104,  0.3265, -2.6117, -4.4250,  3.7640,  0.7856,\n",
      "          0.3645,  0.2973, -1.6184, -2.8684, -3.8761,  3.4028,  3.8132,  3.2711,\n",
      "         -2.9112, -2.0238, -3.4115, -2.2814, -4.8489, -1.5372, -3.6407,  0.6050,\n",
      "         -0.3523, -1.7729, -1.6066, -1.7154,  2.6027,  3.8007,  0.9245,  5.5246,\n",
      "          1.3294, -0.3880,  2.6210, -3.7649, -0.2299,  4.2250,  0.3954, -1.5569,\n",
      "          1.0801,  3.8845,  1.4220,  2.2160, -1.0630,  1.7512, -0.3989, -3.1901,\n",
      "         -1.1478, -1.2804, -0.9189,  0.8990,  0.8305, -1.0718, -1.3060,  2.9170,\n",
      "          0.2441, -0.8890, -0.8105,  0.6225, -3.8519, -1.0982, -0.8845, -1.9599,\n",
      "         -0.2749,  0.9076, -1.5972,  2.0282,  1.4426, -4.3261, -0.3162, -3.5130,\n",
      "          0.3083, -0.9938, -0.7757, -6.7597, -0.8260,  2.0479,  2.3806, -2.0259,\n",
      "          5.1058, -2.5514, -1.7617, -0.9167, -1.8837, -1.1252,  2.5162,  0.7838,\n",
      "         -2.4596,  0.5898,  2.5471, -3.4538, -2.8927,  5.3480, -0.8399, -0.4227,\n",
      "          3.9611,  0.0213,  1.2794, -0.0733, -1.4715,  4.1462, -4.8602, -3.9730,\n",
      "          2.9431,  0.9675,  2.5204,  0.7342, -0.9834, -1.7803, -4.4325,  2.5599,\n",
      "         -1.3960,  0.0661, -2.3135, -0.2884,  0.6131, -1.8405, -1.0036, -0.1456,\n",
      "          2.1328, -0.8051, -1.8822, -0.6900, -0.9701, -1.1171, -1.5076,  3.6218,\n",
      "          3.1398,  1.1166, -0.5249,  3.5760,  1.5926,  1.2611, -0.9913, -1.0718,\n",
      "         -1.6177, -0.0754, -0.3634, -1.0028, -0.0186,  2.5962, -1.4051, -1.1590,\n",
      "         -2.1497,  0.8092, -5.4342,  4.9348, -0.2497,  1.8283, -2.3890,  2.0815,\n",
      "         -4.0613,  1.1958,  1.4658, -3.0034,  1.6696,  1.4500,  1.7250, -0.1280,\n",
      "         -0.2972, -1.1411,  0.2463, -3.9080,  0.3874, -3.4985, -5.3312, -0.7318,\n",
      "         -0.2321,  1.6132, -0.5031,  0.2413,  2.7800, -1.8677, -2.6397,  5.7686,\n",
      "          2.1578,  2.1525,  2.4773, -2.4405, -0.4935, -1.5942, -3.0574,  1.3908,\n",
      "          4.5080,  1.9133,  1.9821,  2.0489, -4.9808,  4.6789,  0.1274,  3.3208,\n",
      "          1.2539,  3.3576,  3.4146,  2.2846,  2.9942, -2.2818, -2.8946, -0.1177,\n",
      "          1.5988,  0.4082, -0.2719,  0.0757,  0.7737,  1.4438,  4.9766, -0.6852,\n",
      "         -1.4440,  1.6450,  1.3066,  5.1186,  1.0681, -1.7604,  0.6715, -1.9538,\n",
      "          2.4373,  0.8411,  0.1357,  0.6477,  1.2750,  1.6689,  0.6223,  2.1225,\n",
      "         -3.3267, -3.0050, -6.0564, -3.3891, -0.3908, -0.3680,  0.0417, -0.7800,\n",
      "          3.2374, -0.2509,  0.9909, -3.8637, -0.8242, -1.5467, -2.1187,  2.3910,\n",
      "         -0.8440, -2.0130, -0.9005, -0.7947,  3.0475,  0.6136, -3.3836, -1.0938,\n",
      "         -0.8890, -1.5339, -3.3976,  1.7789, -0.4531, -3.4086,  0.8243,  2.9002,\n",
      "          2.5724, -3.0283,  3.0786, -1.4545, -0.5932, -3.3968,  0.2788, -3.0285,\n",
      "         -1.1358, -4.0096, -0.9899, -0.9496, -1.2226, -3.8602,  0.4705,  2.4373,\n",
      "          3.1123,  0.3446, -2.0478,  2.1490,  0.6359, -0.4056, -2.3673,  0.1236,\n",
      "          1.7782, -0.9252, -2.9852,  0.4456, -3.4459, -0.5698, -5.9932,  1.7218,\n",
      "          1.4311, -1.0021,  0.7377, -0.8870,  0.5298,  0.6200,  0.2713, -4.9220,\n",
      "          0.8244,  3.5643,  1.4349,  1.6248,  2.5644, -0.2453, -1.9650,  0.3826,\n",
      "         -5.3048,  2.0406,  0.9046, -0.3852, -3.4550,  0.0281, -2.1830, -1.4410,\n",
      "         -2.1677, -1.2684, -0.0420,  2.2193,  0.4140,  2.9327, -0.4252,  1.9747,\n",
      "          0.2027, -0.5843,  0.1672, -1.4124,  0.1676, -0.2104, -0.2699,  0.8030]])\n",
      "ğŸ¯ Chá»‰ sá»‘ dá»± Ä‘oÃ¡n: 0\n",
      "ğŸ“Œ Dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng: Ä‘áº¥m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TimesformerForVideoClassification.from_pretrained(\"custom_timesformer\").to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(\"custom_timesformer\")\n",
    "\n",
    "\n",
    "labels = [\"Ä‘áº¥m\", \"Ä‘Ã¡\", \"tÃ¡t\"]\n",
    "\n",
    "def extract_frames(video_path, num_frames=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames < num_frames:\n",
    "        print(f\"âš  Video {video_path} quÃ¡ ngáº¯n!\")\n",
    "        return None\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Chuyá»ƒn vá» RGB\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    return frames if len(frames) == num_frames else None\n",
    "\n",
    "def predict_action(video_path):\n",
    "    print(f\"ğŸ“‚ Äang xá»­ lÃ½ video: {video_path}\")\n",
    "    frames = extract_frames(video_path, num_frames=8)\n",
    "    \n",
    "    if frames is None:\n",
    "        print(\"âš  Video quÃ¡ ngáº¯n hoáº·c lá»—i khi trÃ­ch xuáº¥t frames.\")\n",
    "        return\n",
    "\n",
    "    inputs = processor(images=frames, return_tensors=\"pt\")\n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(device)  # ÄÆ°a dá»¯ liá»‡u lÃªn GPU\n",
    "\n",
    "    print(\"âœ… ÄÃ£ tiá»n xá»­ lÃ½ xong, báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    print(\"ğŸ”¢ GiÃ¡ trá»‹ logits:\", logits)\n",
    "    print(\"ğŸ¯ Chá»‰ sá»‘ dá»± Ä‘oÃ¡n:\", predicted_class)\n",
    "\n",
    "    if 0 <= predicted_class < len(labels):\n",
    "        print(\"ğŸ“Œ Dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng:\", labels[predicted_class])\n",
    "    else:\n",
    "        print(\"âš  Lá»—i: Chá»‰ sá»‘ dá»± Ä‘oÃ¡n ngoÃ i pháº¡m vi!\", predicted_class)\n",
    "\n",
    "predict_action(\"V_797.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
